{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import argparse\n",
    "from pylab import *\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from model import UNet\n",
    "import SimpleITK as sitk\n",
    "from scipy import ndimage\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from data import SegThorDataset, Rescale, ToTensor, Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data path\n",
    "TRAIN_PATH = 'data/train'\n",
    "TEST_PATH = 'data/test'\n",
    "epochs = 5\n",
    "data = []\n",
    "label = []\n",
    "#train_data = []\n",
    "#test_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(datapath, train):\n",
    "    if train == 1: \n",
    "        for patient in os.listdir(datapath):\n",
    "            image = os.path.join(datapath, patient, patient+'.nii.gz')   # Reading nifti image\n",
    "            itkimage = sitk.ReadImage(image)\n",
    "            \n",
    "            label_img = os.path.join(TRAIN_PATH, patient, 'GT.nii.gz')       # Reading Ground Truth labels\n",
    "            label_itkimage = sitk.ReadImage(label_img)\n",
    "            \n",
    "\n",
    "            # Convert the image to a  numpy array first and then shuffle the dimensions to get axis in the order z,y,x\n",
    "            label_volume_array = sitk.GetArrayFromImage(label_itkimage)\n",
    "            volume_array = sitk.GetArrayFromImage(itkimage)\n",
    "\n",
    "            for s in range(1,volume_array.shape[0]):\n",
    "                # Appending nifti images into an array\n",
    "                slice_array = volume_array[s-1,:,:]\n",
    "                resize_img = ndimage.interpolation.zoom(slice_array, zoom=0.25)   #Resize image  for faster training\n",
    "                data.append(resize_img)\n",
    "                \n",
    "                # Appending ground truth labels into an array\n",
    "                label_slice_array = label_volume_array[s-1,:,:]\n",
    "                label_resize_img = ndimage.interpolation.zoom(label_slice_array, zoom=0.25)   #Resize image  for faster training\n",
    "                label.append(label_resize_img)\n",
    "        return data,label\n",
    "                \n",
    "    else:\n",
    "        for patient in os.listdir(datapath):\n",
    "            filename = os.path.join(datapath, patient, patient+'.nii.gz')\n",
    "            itkimage = sitk.ReadImage(filename)\n",
    "\n",
    "            # Convert the image to a  numpy array first and then shuffle the dimensions to get axis in the order z,y,x\n",
    "            volume_array = sitk.GetArrayFromImage(itkimage)\n",
    "\n",
    "            for s in range(1,volume_array.shape[0]):\n",
    "                slice_array = volume_array[s-1,:,:]\n",
    "                resize_img = ndimage.interpolation.zoom(slice_array, zoom=0.25)   #Resize image  for faster training\n",
    "                data.append(resize_img)\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating class for model (later add it to seperate python file)\n",
    "def train(epochs, batch_size, learning_rate):\n",
    "    #train_loader = torch.utils.data.DataLoader(train_data, batch_size=4, shuffle=True)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        SegThorDataset(\"data\", train=1,\n",
    "                       transform=transforms.Compose([\n",
    "                           Rescale(0.25),\n",
    "                           ToTensor()\n",
    "                       ]),\n",
    "                       target_transform=transforms.Compose([\n",
    "                           Rescale(0.25),\n",
    "                           ToTensor()\n",
    "                       ])),\n",
    "        batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = UNet().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)    #learning rate to 0.001 for intial\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, epochs))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        for batch_idx, (train_data, labels) in enumerate(train_loader):\n",
    "            train_data, labels = train_data.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(train_data)\n",
    "            loss = F.binary_cross_entropy(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        print(\"Loss: {:.4f}\\n\".format(epoch_loss))\n",
    "\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    torch.save(model, \"models/model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## if __name__ == \"__main__\":\n",
    "  #  train_data = read_data(TRAIN_PATH, train = 1)\n",
    " #  labels = read_labels(TRAIN_PATH)\n",
    " #   print('train_data shape:', np.array(train_data).shape)\n",
    " #   print('labels shape:', np.array(labels).shape)\n",
    "    \n",
    "    train(epochs=2, batch_size=4, learning_rate=0.001)\n",
    "    \n",
    "  #  test_data = read_data(TEST_PATH, train = 0)\n",
    "\n",
    "    #print('train_data shape:', np.array(train_data).shape)\n",
    "    #print('test_data shape:', np.array(test_data).shape)\n",
    "#    parser = argparse.ArgumentParser()\n",
    " #   parser.add_argument(\"-d\", \"--datapath\")\n",
    "  #  load_itk(filename=args.filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
